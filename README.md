a. A brief overview of how your code is structured.
b. How to compile, set up, deploy, and use your system.
c. Any limitations in your current implementation (e.g. any assumptions you made;
anything I should not do to break your system, etc.).

Code is structured as a script that begins with reading in a .csv file—this particular instance uses data from the r/pfizervaccine subreddit scraped and formatted by Gabriel Preda (link to dataset: https://www.kaggle.com/gpreda/pfizer-vaccine-on-reddit), but it can applied to other datasets as long as the format of the file is the same. Once the body of the posts has been read in, it then pos tags and uses NER to find frequently referenced entities. Afterwards, it selects user input for criteria such as desired noun and sentiment to generate text around. Then it runs sentiment analysis in tandem w/ dependency parsing to extract sentiment-specific modifiers. Next it uses two regexes on pos tagging data to extract sentiment-specific verb phrases and preposition phrases. Then it uses probabilistic CFG rules to generate text.

In order to use this code, please be sure to download the 'en_core_web_sm' spacy model; this can be done with './python -m spacy download en_core_web_sm' in the project directory  using terminal/shell. Also be sure to install the required packages, as well as the nltk resources (will auto-download in program, except if you have ssl issues).

In listing the entities/nouns frequently referenced in the discourse that weren't fluff, I assumed the user would make a sensible choice in choosing an actual topical noun or entity to generate text around. In addition, I also assumed that the verb phrases used for generation would sensibly fit with the selected noun—this varies across many instances of generated text, but since it is modeled for and after internet discourse it is more acceptable in being less formal and not as grammatically sound. 